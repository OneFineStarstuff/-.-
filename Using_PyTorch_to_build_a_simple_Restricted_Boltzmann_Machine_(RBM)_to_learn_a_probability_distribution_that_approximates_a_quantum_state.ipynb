{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOTCqNk8JuqJkWkR1TIv4ev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Onefinebot/blob/main/Using_PyTorch_to_build_a_simple_Restricted_Boltzmann_Machine_(RBM)_to_learn_a_probability_distribution_that_approximates_a_quantum_state.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbNQmLMR_WAy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_dim, hidden_dim):\n",
        "        super(RBM, self).__init__()\n",
        "        self.visible_dim = visible_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Initialize weights and biases\n",
        "        self.W = nn.Parameter(torch.randn(hidden_dim, visible_dim) * 0.01)\n",
        "        self.b_v = nn.Parameter(torch.zeros(visible_dim))\n",
        "        self.b_h = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    # Forward pass to calculate hidden layer probabilities\n",
        "    def sample_hidden(self, v):\n",
        "        p_h = torch.sigmoid(torch.matmul(v, self.W.t()) + self.b_h)\n",
        "        h = torch.bernoulli(p_h)  # Sample hidden layer\n",
        "        return h\n",
        "\n",
        "    # Forward pass to calculate visible layer probabilities\n",
        "    def sample_visible(self, h):\n",
        "        p_v = torch.sigmoid(torch.matmul(h, self.W) + self.b_v)\n",
        "        v = torch.bernoulli(p_v)  # Sample visible layer\n",
        "        return v\n",
        "\n",
        "    # Contrastive Divergence\n",
        "    def contrastive_divergence(self, v0):\n",
        "        h0 = self.sample_hidden(v0)\n",
        "        v_k = self.sample_visible(h0)\n",
        "        h_k = self.sample_hidden(v_k)\n",
        "\n",
        "        # Update gradients for weights and biases\n",
        "        positive_grad = torch.matmul(h0.t(), v0)\n",
        "        negative_grad = torch.matmul(h_k.t(), v_k)\n",
        "\n",
        "        self.W.grad = (positive_grad - negative_grad) / v0.size(0)\n",
        "        self.b_v.grad = torch.sum(v0 - v_k, dim=0) / v0.size(0)\n",
        "        self.b_h.grad = torch.sum(h0 - h_k, dim=0) / v0.size(0)\n",
        "\n",
        "# Instantiate and train RBM\n",
        "visible_dim = 10  # Number of visible units (could represent spin configuration)\n",
        "hidden_dim = 5    # Number of hidden units\n",
        "rbm = RBM(visible_dim, hidden_dim)\n",
        "optimizer = optim.SGD(rbm.parameters(), lr=0.1)\n",
        "\n",
        "# Generate random training data (could represent measurements of a quantum state)\n",
        "data = torch.bernoulli(torch.rand(100, visible_dim))\n",
        "\n",
        "# Train the RBM using Contrastive Divergence\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    rbm.zero_grad()\n",
        "    v0 = data[torch.randint(0, data.size(0), (1,))]  # Random sample\n",
        "    rbm.contrastive_divergence(v0)\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training complete. RBM parameters trained to approximate a quantum state.\")"
      ]
    }
  ]
}