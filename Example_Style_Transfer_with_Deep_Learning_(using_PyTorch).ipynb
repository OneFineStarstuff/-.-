{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPcsU2to8eEqb+lqL0Be6NX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Onefinebot/blob/main/Example_Style_Transfer_with_Deep_Learning_(using_PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbwJcI-7wc40"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load images (content and style)\n",
        "content_img = Image.open(\"/content/drive/My Drive/Colab Notebooks/your_uploaded_content_image.jpg\").convert('RGB')\n",
        "style_img = Image.open(\"/content/drive/My Drive/Colab Notebooks/your_uploaded_style_image.jpg\").convert('RGB')\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "content_tensor = transform(content_img).unsqueeze(0)\n",
        "style_tensor = transform(style_img).unsqueeze(0)\n",
        "\n",
        "# Load a pre-trained VGG model for style transfer\n",
        "vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.eval()\n",
        "\n",
        "# Function to compute content loss\n",
        "def content_loss(content, target):\n",
        "    return torch.nn.functional.mse_loss(content, target)\n",
        "\n",
        "# Function to compute style loss\n",
        "def gram_matrix(input):\n",
        "    batch_size, feature_maps, h, w = input.size()\n",
        "    features = input.view(batch_size * feature_maps, h * w)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(batch_size * feature_maps * h * w)\n",
        "\n",
        "def style_loss(style, target):\n",
        "    G = gram_matrix(target)\n",
        "    A = gram_matrix(style)\n",
        "    return torch.nn.functional.mse_loss(G, A)\n",
        "\n",
        "# Function to compute style transfer\n",
        "def style_transfer(content, style, model, num_steps=500, style_weight=1e6, content_weight=1):\n",
        "    input_img = content.clone()\n",
        "    optimizer = torch.optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "    content_features = model(content).detach()\n",
        "    style_features = model(style).detach()\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        def closure():\n",
        "            input_img.data.clamp_(0, 1)\n",
        "            optimizer.zero_grad()\n",
        "            input_features = model(input_img)\n",
        "            c_loss = content_weight * content_loss(input_features, content_features)\n",
        "            s_loss = style_weight * style_loss(style_features, input_features)\n",
        "            loss = c_loss + s_loss\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    return input_img\n",
        "\n",
        "# Apply style transfer and display\n",
        "output = style_transfer(content_tensor, style_tensor, vgg)\n",
        "plt.imshow(output.squeeze().permute(1, 2, 0).detach().numpy())\n",
        "plt.show()"
      ]
    }
  ]
}