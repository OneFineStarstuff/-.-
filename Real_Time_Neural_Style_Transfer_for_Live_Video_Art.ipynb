{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNRm8ndaCc6GQ3EHt6hz3dW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Onefinebot/blob/main/Real_Time_Neural_Style_Transfer_for_Live_Video_Art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0e46wLEpi1F"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "# Define the style transfer model\n",
        "class StyleTransferModel:\n",
        "    def __init__(self, style_img_path):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.vgg = vgg19(pretrained=True).features.to(self.device).eval()\n",
        "        self.style_img = self.load_image(style_img_path)\n",
        "\n",
        "    def load_image(self, img_path):\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image).unsqueeze(0)\n",
        "        return image.to(self.device)\n",
        "\n",
        "    def transform(self, frame):\n",
        "        # Convert frame to PIL image\n",
        "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        frame_tensor = self.transform(frame_pil).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Apply style transfer (simplified example, not a full implementation)\n",
        "        stylized_tensor = frame_tensor  # Placeholder for actual style transfer computation\n",
        "        stylized_frame = stylized_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "        stylized_frame = cv2.cvtColor(stylized_frame, cv2.COLOR_RGB2BGR)\n",
        "        return stylized_frame\n",
        "\n",
        "# Initialize the style transfer model with a style image\n",
        "model = StyleTransferModel(\"style.jpg\")\n",
        "\n",
        "def apply_style_to_frame(frame, model):\n",
        "    return model.transform(frame)\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        frame = apply_style_to_frame(frame, model)\n",
        "        cv2.imshow('Live Style Transfer', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "DHgVxyMcpuYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}