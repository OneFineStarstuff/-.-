{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNswrPFO5yslfwm7ZqzbIka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Onefinebot/blob/main/_Deep_Learning_for_Quantum_Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, output_dim),  # Output dimension should be 28*28 for MNIST-like images\n",
        "            nn.Tanh()  # Output in range [-1, 1]\n",
        "        )\n",
        "        # Initialize weights\n",
        "        for m in self.fc:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),  # Output a single scalar\n",
        "        )\n",
        "        # Initialize weights\n",
        "        for m in self.fc:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)  # No sigmoid here, BCEWithLogitsLoss expects raw logits\n",
        "\n",
        "# Initialize Models and Optimizers\n",
        "z_dim = 100  # Latent dimension\n",
        "data_dim = 28 * 28  # Flattened image dimension for MNIST (28x28 = 784)\n",
        "generator = Generator(z_dim, data_dim)\n",
        "discriminator = Discriminator(data_dim)\n",
        "\n",
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Learning rate scheduler (optional)\n",
        "scheduler_g = optim.lr_scheduler.StepLR(optim_g, step_size=30, gamma=0.1)\n",
        "scheduler_d = optim.lr_scheduler.StepLR(optim_d, step_size=30, gamma=0.1)\n",
        "\n",
        "# Loss function (using BCEWithLogitsLoss for stability)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# MNIST Dataset loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Helper function to save generator outputs\n",
        "def save_generator_output(epoch, generator, z_dim, num_images=5):\n",
        "    noise = torch.randn(num_images, z_dim)  # Generate random latent vectors\n",
        "    generated_images = generator(noise).detach().numpy()\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        axes[i].imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    plt.savefig(f'generated_images_epoch_{epoch}.png')  # Save the images\n",
        "\n",
        "# Training Loop\n",
        "epochs = 100\n",
        "\n",
        "# Tracking loss\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for real_batch, _ in train_loader:\n",
        "        real_batch = real_batch.view(-1, data_dim)  # Flatten MNIST images\n",
        "        real_batch_size = real_batch.size(0)  # Get the actual batch size\n",
        "\n",
        "        # Generate fake data with the same batch size\n",
        "        noise = torch.randn(real_batch_size, z_dim)  # Match batch size\n",
        "        fake_data = generator(noise).detach()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optim_d.zero_grad()\n",
        "        real_labels = torch.ones(real_batch_size, 1) * 0.9  # Label smoothing: real data labels (0.9 instead of 1)\n",
        "        fake_labels = torch.zeros(real_batch_size, 1)  # Fake data labels (0)\n",
        "\n",
        "        # Calculate loss for real and fake data\n",
        "        loss_d_real = criterion(discriminator(real_batch), real_labels)\n",
        "        loss_d_fake = criterion(discriminator(fake_data), fake_labels)\n",
        "        loss_d = (loss_d_real + loss_d_fake) / 2  # Average the losses\n",
        "        loss_d.backward()\n",
        "        optim_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optim_g.zero_grad()\n",
        "        # The generator wants to fool the discriminator into thinking its fake data is real\n",
        "        loss_g = criterion(discriminator(generator(noise)), real_labels)  # We want fake data to be labeled as real\n",
        "        loss_g.backward()\n",
        "        optim_g.step()\n",
        "\n",
        "    # Scheduler step (optional)\n",
        "    scheduler_g.step()\n",
        "    scheduler_d.step()\n",
        "\n",
        "    # Track losses\n",
        "    d_losses.append(loss_d.item())\n",
        "    g_losses.append(loss_g.item())\n",
        "\n",
        "    # Print and visualize periodically\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}\")\n",
        "        save_generator_output(epoch, generator, z_dim)  # Save generated images\n",
        "\n",
        "    # Save the model periodically\n",
        "    if epoch % 50 == 0:\n",
        "        torch.save(generator.state_dict(), f'generator_epoch_{epoch}.pth')\n",
        "        torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch}.pth')\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
        "plt.plot(g_losses, label=\"Generator Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.savefig(\"training_losses.png\")\n",
        "\n",
        "# Final model save\n",
        "torch.save(generator.state_dict(), 'generator_final.pth')\n",
        "torch.save(discriminator.state_dict(), 'discriminator_final.pth')"
      ],
      "metadata": {
        "id": "qsabKWuQELjz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}